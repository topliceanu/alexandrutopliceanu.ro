<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    <base href="http://alexandrutopliceanu.ro/">
    

    <link rel="dns-prefetch" href="//fonts.googleapis.com/">

    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Old+Standard+TT:400' rel='stylesheet' type='text/css'>

    <title>
      
      
         Cache Data Structures 
      
    </title>
    <link rel="canonical" href="http://alexandrutopliceanu.ro/post/cache-data-structures/">

    <style>
  * {
    border:0;
    font:inherit;
    font-size:100%;
    vertical-align:baseline;
    margin:0;
    padding:0;
    color: black;
  }

  body {
    font-family:'Open Sans', 'Myriad Pro', Myriad, sans-serif;
    font-size:17px;
    line-height:160%;
    color:#1d1313;
    max-width:700px;
    margin:auto;
  }

  p {
    margin: 5px 0 10px 0;
  }

  a img {
    border:none;
  }

  img {
    margin: 10px auto 10px auto;
    max-width: 100%;
    display: block;
  }

  pre, code {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
    background-color: #f7f7f7;
  }

  code {
    font-size: 12px;
    padding: 4px;
  }

  pre {
    margin-top: 0;
    margin-bottom: 16px;
    word-wrap: normal;
    padding: 16px;
    overflow: auto;
    font-size: 85%;
    line-height: 1.45;
  }

  pre>code {
    padding: 0;
    margin: 0;
    font-size: 100%;
    word-break: normal;
    white-space: pre;
    background: transparent;
    border: 0;
  }

  pre code {
    display: inline;
    max-width: auto;
    padding: 0;
    margin: 0;
    overflow: visible;
    line-height: inherit;
    word-wrap: normal;
    background-color: transparent;
    border: 0;
  }

  pre code::before,
  pre code::after {
    content: normal;
  }

  em,q,em,dfn {
    font-style:italic;
  }

  .sans,html .gist .gist-file .gist-meta {
    font-family:"Open Sans","Myriad Pro",Myriad,sans-serif;
  }

  .mono,pre,code,tt,p code,li code {
    font-family:Menlo,Monaco,"Andale Mono","lucida console","Courier New",monospace;
  }

  .heading,.serif,h1,h2,h3 {
    font-family:"Old Standard TT",serif;
  }

  strong {
    font-weight:600;
  }

  q:before {
    content:"\201C";
  }

  q:after {
    content:"\201D";
  }

  del,s {
    text-decoration:line-through;
  }

  blockquote {
    font-family:"Old Standard TT",serif;
    text-align:center;
    padding:50px;
  }

  blockquote p {
    display:inline-block;
    font-style:italic;
  }

  blockquote:before,blockquote:after {
    font-family:"Old Standard TT",serif;
    content:'\201C';
    font-size:35px;
    color:#403c3b;
  }

  blockquote:after {
    content:'\201D';
  }

  hr {
    width:40%;
    height: 1px;
    background:#403c3b;
    margin: 25px auto;
  }

  h1 {
    font-size:35px;
  }

  h2 {
    font-size:28px;
  }

  h3 {
    font-size:22px;
    margin-top:18px;
  }

  h1 a,h2 a,h3 a {
    text-decoration:none;
  }

  h1,h2 {
    margin-top:28px;
  }

  #sub-header, time {
    color:#403c3b;
    font-size:13px;
  }

  #sub-header {
    margin: 0 4px;
  }

  #nav h1 a {
    font-size:35px;
    color:#1d1313;
    line-height:120%;
  }

  .posts_listing a,#nav a {
    text-decoration: none;
  }

  li {
    margin-left: 20px;
  }

  ul li {
    margin-left: 5px;
  }

  ul li {
    list-style-type: none;
  }
  ul li:before {
    content:"\00BB \0020";
  }

  #nav ul li:before, .posts_listing li:before {
    content:'';
    margin-right:0;
  }

  #content {
    text-align:left;
    width:100%;
    font-size:15px;
    padding:60px 0 80px;
  }

  #content h1,#content h2 {
    margin-bottom:5px;
  }

  #content h2 {
    font-size:25px;
  }

  #content .entry-content {
    margin-top:15px;
  }

  #content time {
    margin-left:3px;
  }

  #content h1 {
    font-size:30px;
  }

  .highlight {
    margin: 10px 0;
  }

  .posts_listing {
    margin:0 0 50px;
  }

  .posts_listing li {
    margin:0 0 25px 15px;
  }

  .posts_listing li a:hover,#nav a:hover {
    text-decoration: underline;
  }

  #nav {
    text-align:center;
    position:static;
    margin-top:60px;
  }

  #nav ul {
    display: table;
    margin: 8px auto 0 auto;
  }

  #nav li {
    list-style-type:none;
    display:table-cell;
    font-size:15px;
    padding: 0 20px;
  }

  #links {
    margin: 50px 0 0 0;
  }

  #links :nth-child(2) {
    float:right;
  }

  #not-found {
    text-align: center;
  }

  #not-found a {
    font-family:"Old Standard TT",serif;
    font-size: 200px;
    text-decoration: none;
    display: inline-block;
    padding-top: 225px;
  }

  @media (max-width: 750px) {
    body {
      padding-left:20px;
      padding-right:20px;
    }

    #nav h1 a {
      font-size:28px;
    }

    #nav li {
      font-size:13px;
      padding: 0 15px;
    }

    #content {
      margin-top:0;
      padding-top:50px;
      font-size:14px;
    }

    #content h1 {
      font-size:25px;
    }

    #content h2 {
      font-size:22px;
    }

    .posts_listing li div {
      font-size:12px;
    }
  }

  @media (max-width: 400px) {
    body {
      padding-left:20px;
      padding-right:20px;
    }

    #nav h1 a {
      font-size:22px;
    }

    #nav li {
      font-size:12px;
      padding: 0 10px;
    }

    #content {
      margin-top:0;
      padding-top:20px;
      font-size:12px;
    }

    #content h1 {
      font-size:20px;
    }

    #content h2 {
      font-size:18px;
    }

    .posts_listing li div{
      font-size:12px;
    }
  }
</style>


    <link rel="shortcut icon" href="http://alexandrutopliceanu.ro/favicon.ico">

    
  </head>

  <body>
    <section id=nav>
      <h1><a href="http://alexandrutopliceanu.ro/">Alexandru Topliceanu</a></h1>
      <ul>
        
          <li><a href="mailto:alexandru.topliceanu@gmail.com">Email</a></li>
        
          <li><a href="https://github.com/topliceanu">Github</a></li>
        
          <li><a href="https://twitter.com/topliceanu">Twitter</a></li>
        
      </ul>
    </section>


<section id=content>
  <h1> Cache Data Structures </h1>

  <div id=sub-header>
    April 2020 Â· 14 minute read
  </div>

  <div class="entry-content">
    <h2 id="intro">Intro</h2>
<p>In computing tradeoffs between time and space complexity often need to be made.
An solution that is faster at the expense of using more storage is usually preferred.
One way to make a solution faster is to cache results for expensive operations.
Since cache capacity is limitted and is usually much smaller than the amount of
data the algorithm requires, caching evey result is not practical.
Once the cache memory is at capacity, the caching system needs to decide which
piece of data to remove so it can make room for new pieces of data.
This is the cache replacement problem. <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies" title="This Wikipedia page describes a large list of cache replacement strategies. It's beautiful!">1</a></p>
<p>This article goes through several strategies for cache replacement.
It&rsquo;s accompanied by example implementations in golang with benchmarks for speed,
memory usage and cache miss rates. <a href="https://github.com/topliceanu/cache" title="My Golang implementation for the cache data structures described above">2</a></p>
<h2 id="the-caching-replacement-problem">The caching replacement problem</h2>
<p>Given two types of memory, a fast but small one and a slow but large one,
the aim is to design a system with the performance of the first and the capacity of the second.
For both memory types, data is split into addressable chunks of data, called <em>pages</em>.
A storage system transparently builds a cache of page copies from the slow memory in the fast memory.
When the client requests a page that is in the cache - a <em>cache hit</em>, the storage
system will read the page from the fast memory and return it. Otherwise, in case of
a <em>cache miss</em>, the system will fetch the page from the backend slow memory, write it in the
fast memory and return it to the client. See <em>Figure 1.a</em>.</p>
<pre><code>client       cache        backend          client       cache        backend
--+---       --+--        ---+---          --+---       --+--        ---+---
  |            |             |               |            |             |
  |  read key  | (miss)      |               |  read key  |             |
  H-----------&gt;H             |               H-----------&gt;H             |
  H            H  read key   |               H    miss    H             |
  H            H------------&gt;H               H&lt;-----------H             |
  H            H    val      H               H            |             |
  H            H&lt;------------H               H         read key         |
  H            H             |               H-------------------------&gt;H
  H    val     H (store val) |               H            |             H
  H&lt;-----------H             |               H           val            H
  |            |             |               H&lt;-------------------------H
                                             H            |             |
              (a)                            H  write val |             |
                                             H------------H             |
                                             H    ok      H             |
                                             H&lt;-----------H             |
                                             |            |             |

                                                         (b)
</code></pre><p><em>Figure 1: a. cache miss in a storage system; b. cache miss in a general system</em></p>
<p>The storage system approach couples the cache with the backend storage.
In a general purpose cache, the pages can come from something other than a slow disk,
for instance, they can be results of expensive computations or a network calls.
In this approach, the decision to write pages to the cache is left to the client.
See <em>Figure 1.b</em>.</p>
<pre><code>Read(key int) (value int, isCacheMiss bool)
Write(key, value int)
</code></pre><p><em>Listing 1: general purpose cache interface</em></p>
<p>In the following sections, this document goes through several cache page replacement strategies.</p>
<h2 id="lru---least-recently-used">LRU - Least recently used</h2>
<p>The observation at the base of LRU is that whatever page was requested
recently has the highest chance to be requested again in the near future - the data exibits
<em>locallity of reference</em>. Conversely, the data in the cache that hasn&rsquo;t been
requested for the longest time, has the least chance of being requested again the near future.
This is the data that LRU evicts to make room for new pages.</p>
<p>Cache performance depends on the use-case, but, in practice, LRU works well enough
in many situations, it&rsquo;s very fast and easy to implement.</p>
<p>It is implemented with a double-linked list and a hash map.
The hash map gives constant lookup times for data keys, whereas the list maintains
the invariant that the nodes that were requested recently are at the head of the list.
Discarding the last used node as well as promoting recently accessed nodes is
as easy as updating a few pointers in constanst time.</p>
<pre><code>+------+---------------+-----------+------------------------------------------------------+
| step | state (list)  | operation | effect                                               |
+------+---------------+-----------+------------------------------------------------------+
| 1    | ()-&gt;()-&gt;()    | write 3   | add 3 to the empty cache                             |
| 2    | (3)-&gt;()-&gt;()   | write 2   | add 2 before 3, capacity not reached                 |
| 3    | (2)-&gt;(3)-&gt;()  | write 1   | add 1 before 2, capacity not reached                 |
| 4    | (1)-&gt;(2)-&gt;(3) | write 4   | add 4 before 1 and evict 3 since the cache overflowed|
| 5    | (4)-&gt;(1)-&gt;(2) | write 5   | add 5 before 4, evit 2 since its the LRU node        |
| 6    | (5)-&gt;(4)-&gt;(1) | read 1    | read 1 bumps it to the front of the list, the MRU    |
| 7    | (1)-&gt;(5)-&gt;(4) |           |                                                      |
+------+---------------+-----------+------------------------------------------------------+
</code></pre><p><em>Table 1: an example of operations and their effect of the LRU cache</em></p>
<h2 id="mru---most-recently-used">MRU - Most recently used</h2>
<p>In the case of repeatedly scanning large pieces of data or accessing random
pages from a very large memory, LRU does not perform very well.
It has been proven - see <a href="http://code.activestate.com/recipes/576532-adaptive-replacement-cache-in-python/" title="ARC implementation in python">8</a> - that evicting the most recently used page has better
cache hit rates in this case.</p>
<p>The implementation is very similar to LRU - the most recently used key is at the head of the list
and the least recently used key is at the end - except that in MRU, the key that is
read last - the head of the linked list -  is also the key that is evicted before the next write.</p>
<h2 id="lfu---least-frequently-used">LFU - Least frequently used</h2>
<p>The intuition for LFU is to evict the page that has been requested the fewest
times out of all the pages in the cache.</p>
<p>In practice, maintaining the complete history of key accesses is not possible, so
most implementations use an aproximate LFU (ALFU) where the access history is
retained for a limitted time window.</p>
<p>Much like LRU, this implementation also uses a hash map for fast access to a given key.
Unlike LRU, LFU needs to maintain a of all the keys in the cache sorted by frequency of access.
A double-linked list would not work because the list needs to be kept sorted,
yielding a O(n) complexity on every operation.</p>
<p>A heap is a good candidate because pages don&rsquo;t need to be strictly sorted by frequency,
the algorithm only requires the least accessed page for eviction. Writing keys
as well as reading and promoting keys in a heap takes O(n) in the worst case.</p>
<p>In general when choosing a data structure for an algorithm, the goal is to find
the structure that exposes no more functionality than what is needed.
Generally speaking, the more extra functionality a data structure has, the more
expensive its operations are going to be - in both time and space complexity.</p>
<p>To keep things simple, the implementation in the accompanying repository <a href="https://github.com/topliceanu/cache" title="My Golang implementation for the cache data structures described above">2</a> only
takes into account the number of hits, not their frequency.
This is not accurate, since there is a pathological worst case use-case where a key
that was requested many times in just one day a year ago is likely to still be in the
cache even if it hasn&rsquo;t been requested since.</p>
<pre><code>+------+---------------------+-----------+--------------------------------------------------------------+
| step | state (key, freq)   | operation | effect                                                       |
+------+---------------------+-----------+--------------------------------------------------------------+
| 1    | ()-&gt;()-&gt;()          | write 3   | add 3 to the empty cache                                     |
| 2    | (3,1)-&gt;()-&gt;()       | write 2   | add 2 before 3, capacity not reached                         |
| 3    | (2,1)-&gt;(3,1)-&gt;()    | write 1   | add 1 before 2, capacity not reached                         |
| 4    | (1,1)-&gt;(2,1)-&gt;(3,1) | read 2    | read 2 bumps its access count to 2, move to back of the heap |
| 5    | (1,1)-&gt;(3,1)-&gt;(2,2) | read 1    | read 1 bumps its access count to 2, move to back of the heap |
| 6    | (3,1)-&gt;(2,2)-&gt;(1,2) | write 5   | write 5, evicts 3 because its lowest access count            |
| 7    | (1,2)-&gt;(2,2)-&gt;(5,1) |           |                                                              |
+------+---------------------+-----------+--------------------------------------------------------------+
</code></pre><p><em>Table 2: an example of operations and their effect of the LFU cache</em></p>
<h2 id="scan-resistence">Scan resistence</h2>
<p>One of the main disadvantage of LRU is that it is not scan resistent.
To understand this property, consider an LRU cache with a capacity of n pages.
Over time, the data in the cache will capture the frequency of access to keys:
more popular keys will be near the front of the linked list, the less popular
ones to the back.</p>
<p>Consider the case when a client writes a sequence of n all-new pages to cache.
All the existing values in the cache will be removed and replaced with the recent writes.
The implicit key access frequency information is lost.</p>
<p>For a concrete example, a database cache, where table scans are common, must be
resistent to scans in order to perform well. Reference <a href="https://medium.com/@often_weird/what-makes-mysql-lru-cache-scan-resistant-a73364f286d7" title="Blog post about the techniques used in MySQL to make the cache scan resistent">11</a> is a blog post
about how MySQL uses a modified LRU cache that is scan resistent.
It is similar to SLRU, which we will talk about next.</p>
<h2 id="slru">SLRU</h2>
<p>This cache splits its allocated memory into two LRU segments: probation and protected.
When a piece of data is written to the cache, it is first stored in the probation section.
When a key is read from the cache, it is moved to the head of the protected
segment, irrespective of whether it was located in protected or the probation sections.</p>
<p>When overflow occurs on the probation segment, the evicted key
is dumped completely from the cache. When overflow occurs on the protected
segment, the evicted key is moved to the probation segment,
giving it another chance to be promoted to the protected section with another read in the future.</p>
<pre><code>      #     |    Protected LRU                  Probation LRU    |  Operations
------------+----------------------------------------------------+----------------------------------------
     1.     |     -----------                   -------------    | d is evicted from the Protected
  eviction  |      |a|b|c|d|                     | |x|y|z|t|     | section and it is added into the
  from      |     --------|--                   --^----------    | head of the Probation section.
  Probation |             |                       |              |
            |             +-----------------------+              |
------------+----------------------------------------------------+-----------------------------------------
     2.     |     -----------                   -------------    | z is read from the Probation section.
 hit in     |      | |a|b|c|                     | |x|y|z|t|     | It is moved to the head of Protected.
 Probation  |     --^-----|--                   --^-----|----    | section. This triggers c to be evicted
            |       |     +-----------------------+     |        | and moved to the head of Probation.
            |       +-----------------------------------+        |
------------+----------------------------------------------------+-----------------------------------------
     3.     |     -----------                   -------------    | c is read from the Protected section.
 hit in     |      | |a|b|c|                     | | | | | |     | It is moved to the head of the
 Protected  |     --^-----|--                   -------------    | Protected section.
            |       |     |                                      |
            |       +-----+                                      |
------------+----------------------------------------------------+-----------------------------------------
            |
</code></pre><p><em>Table 3: SLRU data movement under various conditions</em></p>
<p>In the event of a scan, it&rsquo;s only the probation section that is flushed.
Because it keeps keys that were requested at least 2 time separately, the
protected section can now maintain the implicit frequence information.</p>
<p>This strategy works better when there&rsquo;s a lot of churn on the accessed values.
The ratio betwen the protected and probation sections can also be a tunable parameter.</p>
<h2 id="lfru---least-frequent-recently-used">LFRU - Least Frequent Recently Used</h2>
<p>Resistance to a single scan proves very useful in the general case, but for databases,
where table scans are the norm, if two table scans happen one after the other then
the same problem occurs.</p>
<p>A variation of SLRU is LFRU, or Least Frequent Recently Used.
It also is split in two sections, this time called privileged (LRU) and unprivileged (LFU).
The aim is to promote to the priviledged section only the pages that have a
higher rate of requests, thus filtering out locally popular pages.</p>
<p>When a cache miss occurs, the client will write the data page in the cache. It will be
stored in the unpriviledged section - a max heap sorted by frequency of access -
at the back of the heap. Popular items are promoted from
the unpriviledged section into the LRU privileged section.</p>
<h2 id="arc---adaptive-replacement-cache">ARC - Adaptive replacement cache</h2>
<p>Much like SLRU in the previous section, the idea with ARC is to improve on LRU.
To outperform LRU, an algorithm needs to adapt to the changing workload with
minimal pre-tuning as well as maintaining LRU&rsquo;s minimal overhead.</p>
<p>ARC outperform LRU by automatically adapting between a frequency and a recency
cache. ARC continuously adapts the size of each cache depending on the workload.
It does not use LFU for the frequency section, instead it maintains two LRU lists,
L1 and L2. L1 holds items that have been requested only once, while L2 hold items
that were requested at least twice, in a sense, functioning as a frequency cache.</p>
<p>Now for the adaptive part. ARC also maintains two more lists b1 and b2, associated
with T1 and T2 respectively. Whenever a page is evicted from t1 it is recorded
in b1. Likewise evictions from t2 are added to b2. B1 and b2 are not part of the
cache, they only hold metadata of the evicted pages. When more pages are evicted
from t1 than t2, the cache will rebalance to give more memory to t1 and vice-versa.</p>
<pre><code>    . . . [   b1  &lt;-[     t1    &lt;-!-&gt;      t2   ]-&gt;  b2   ] . . .
          [ . . .  l1 . . . . . . ! . .^. . . . l2  . . . ]
                    [   fixed cache size (c)    ]
</code></pre><p><em>Figure 2. ARC data structure</em></p>
<p>ARC and variants of it are widely used in the industry, most notably ZFS,
Postgresql and VMWare vSAN.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>The performance indicators of the cache are the speed of reads and writes,
the extra memory used to store the data and the cache hit rate
(number of hits / total number of requests). The cache hit rate is application specific.
Readers are encouraged to try multiple cache implementations for their specific
workload before deciding which variant to use.</p>
<p>Inspired by <a href="https://github.com/Xeoncross/go-cache-benchmark" title="For production use, David Pennington made a benchmark to compare different golang cache implementations">3</a>, the accompanying implementation <a href="https://github.com/topliceanu/cache" title="My Golang implementation for the cache data structures described above">2</a> contains benchmarks
for all algorithms described above:</p>
<pre><code>$ ./script/benchmark
goos: linux
goarch: amd64
pkg: github.com/topliceanu/cache/benchmark
BenchmarkCache/cache-lru/Write()-4       3983329     293.0 ns/op     32 B/op     1 allocs/op
BenchmarkCache/cache-lru/Read()-4       38942666      30.9 ns/op      0 B/op     0 allocs/op
BenchmarkCache/cache-lfu/Write()-4       5492858     229.0 ns/op     31 B/op     0 allocs/op
BenchmarkCache/cache-lfu/Read()-4       36854689      30.5 ns/op      0 B/op     0 allocs/op
BenchmarkCache/cache-mru/Write()-4       5892952     217.0 ns/op     31 B/op     0 allocs/op
BenchmarkCache/cache-mru/Read()-4      174675004       6.9 ns/op      0 B/op     0 allocs/op
BenchmarkCache/cache-slru/Write()-4      3704128     339.0 ns/op     32 B/op     1 allocs/op
BenchmarkCache/cache-slru/Read()-4      16276162      69.6 ns/op      0 B/op     0 allocs/op
BenchmarkCache/cache-lfru/Write()-4      3918639     316.0 ns/op     32 B/op     1 allocs/op
BenchmarkCache/cache-lfru/Read()-4      16141981      65.3 ns/op      0 B/op     0 allocs/op
BenchmarkCache/cache-arc/Write()-4       1901154     634.0 ns/op     65 B/op     2 allocs/op
BenchmarkCache/cache-arc/Read()-4       12199518      88.9 ns/op      0 B/op     0 allocs/op
PASS
ok      github.com/topliceanu/cache/benchmark   19.684s
</code></pre><p><em>Listing x: Read/Write benchmarks for the cache implementations</em></p>
<p>As expected the more complex caches take more time, eg. arc take more than twice the time and memory per op.</p>
<p>Picking the appropriate cache replacement strategy depends on the access distribution
of data in each application. The accompanying project to this blog post has a tool
that generates random data and calculates hit and miss rates:</p>
<pre><code>$ go run cmd/hit-rate/main.go
Cache type    Hit rate   Miss rate
 cache-lru    9.990      90.010
 cache-lfu    9.997      90.003
 cache-mru    9.993      90.007
cache-slru    9.934      90.066
cache-lfru    9.992      90.008
 cache-arc    9.941      90.059
</code></pre><p>This test builds up a data set of one million integers, with cardinality of 10000.
Each instance of a caching algorithm is initialized with 1000 capacity.
For each of the one million values, are page read is performed, if a cache miss
is returned then a page write with that value is performed.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The perfect cache solution was described by Belady in 1960 and states that the
best value to discard is the one that you know you are going to need futhest
in the future. While intuitive, it is not possible to implement it. In general,
we cannot know when a piece of data will be requested in the future.
Belady&rsquo;s contribution, however, is to produce a formal prof that this indeed the
optimal caching algorithm. It&rsquo;s purpose is to act as the ideal benchmark for
comparison for other implementations.</p>
<p>When designing your cache implementation, you want to take into consideration the
historical data access patterns for your specific use-case, if the extra effort
to design and implement it is reasonable.</p>
<h2 id="end-notes">End notes</h2>
<p>Thank you for getting this far. I hope you learned something new.
I plan to explore ARC some more and come back with a comprehensive description.</p>
<p>Please leave comments on <a href="">Hacker News</a>, on <a href="">Lobste.rs</a> or on <a href="">Reddit</a>.</p>
<p>The source for this post is on <a href="https://github.com/topliceanu/alexandrutopliceanu.ro">github.com/topliceanu/alexandrutopliceanu.ro</a>,
If you open an issue or a PR to make improvements, I would very much appreciate it!</p>
<h2 id="resources">Resources</h2>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">link</a> &ldquo;This Wikipedia page describes a large list of cache replacement strategies. It&rsquo;s beautiful!&rdquo;</li>
<li><a href="https://github.com/topliceanu/cache">link</a> &ldquo;My Golang implementation for the cache data structures described above&rdquo;</li>
<li><a href="https://github.com/Xeoncross/go-cache-benchmark">link</a> &ldquo;For production use, David Pennington made a benchmark to compare different golang cache implementations&rdquo;</li>
<li><a href="https://youtu.be/Dh7vmvk9huM">link</a> &ldquo;Professor Tim Roughgarden, in his popular MOOC &lsquo;Introduction to Algorithms, Part 2&rsquo;, has a section where he introduces the caching problem and Belady&rsquo;s ideal cache algorithm.&rdquo;</li>
<li><a href="https://arxiv.org/pdf/1702.04078.pdf">link</a> &ldquo;LFRU paper&rdquo;</li>
<li><a href="https://lrita.github.io/images/posts/datastructure/ARC.pdf">link</a> &ldquo;Not the original ARC paper but a simplified version written by the same authors&rdquo;</li>
<li><a href="https://en.wikipedia.org/wiki/Adaptive_replacement_cache">link</a> &ldquo;ARC wikipedia page is very good if you want a TL;DR&rdquo;</li>
<li><a href="http://code.activestate.com/recipes/576532-adaptive-replacement-cache-in-python/">link</a> &ldquo;ARC implementation in python&rdquo;</li>
<li><a href="http://www.vldb.org/conf/1985/P127.PDF">link</a> &ldquo;MRU paper with experimental data of when it perform better than LRU&rdquo;</li>
<li><a href="https://pdfs.semanticscholar.org/eacf/df93e03d9dbbbaa2d01250939d9f94fb16a4.pdf">link</a> &ldquo;Belady&rsquo;s MIN optimal cache paper&rdquo;</li>
<li><a href="https://medium.com/@often_weird/what-makes-mysql-lru-cache-scan-resistant-a73364f286d7">link</a> &ldquo;Blog post about the techniques used in MySQL to make the cache scan resistent&rdquo;</li>
</ol>

  </div>

  <div id=links>
    
      <a class="basic-alignment left" href="http://alexandrutopliceanu.ro/post/targeted-quantiles/">&laquo; Targeted Quantiles in Prometheus</a>
    
    
  </div>
</section>

  </div>
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-24987736-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</body>
</html>



