<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>alexandru topliceanu</title>
    <link>http://alexandrutopliceanu.ro/index.xml</link>
    <description>Recent content on alexandru topliceanu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 11 Feb 2017 15:16:45 +0000</lastBuildDate>
    <atom:link href="http://alexandrutopliceanu.ro/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>quality go code</title>
      <link>http://alexandrutopliceanu.ro/post/quality-go-code/</link>
      <pubDate>Sat, 11 Feb 2017 15:16:45 +0000</pubDate>
      
      <guid>http://alexandrutopliceanu.ro/post/quality-go-code/</guid>
      <description>

&lt;h1 id=&#34;quality-golang-code&#34;&gt;Quality Golang Code&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;The tools I use to help produce quality Golang code&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Go has excellent libraries for source-code parsing. These have enabled the creators of Go and the open source community to produce a variety of tools which help eliminate errors in advance.&lt;/p&gt;

&lt;p&gt;The large number of tools and the lack of documentation on how to best use them presents a problem for engineers.
This blog post describes the process and the tools I use to improve code quality and catch bugs early in my Go code.
I will go through linting, testing, code coverage, benchmarking, race detection in concurrent code, memory, CPU and contention profiling.&lt;/p&gt;

&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;An HTML version can be generated dynamically from a package&amp;rsquo;s source code.
Running &lt;code&gt;godoc -http=:8080&lt;/code&gt; will start a static HTTP server on &lt;code&gt;0.0.0.0:8080&lt;/code&gt; which will generate on the fly and serve the documentation for all standard library packages as well as all the packages &lt;code&gt;$GOPATH&lt;/code&gt;.
Go to &lt;code&gt;http://0.0.0.0:8080/pkg/github.com/your/package&lt;/code&gt; to see the live docs of your package as you are developing it. Make sure you document all your exported members.&lt;/p&gt;

&lt;h2 id=&#34;linting&#34;&gt;Linting&lt;/h2&gt;

&lt;p&gt;Code linting is a static analysis technique meant to prevent programmers from doing mistakes commonly seen in a specific language.&lt;/p&gt;

&lt;p&gt;A popular project in Go is &lt;a href=&#34;https://github.com/alecthomas/gometalinter&#34;&gt;go-metalinter&lt;/a&gt;. I found it to be too broad. Instead, I use it as a comprehensive list of &amp;ldquo;best practice&amp;rdquo; tools from which I pick only what I need, depending on the project.
In particular, I use &lt;code&gt;go fmt&lt;/code&gt; because it is the standard way to format source code.
I&amp;rsquo;ve given up on &lt;code&gt;goimport&lt;/code&gt; (which also runs &lt;code&gt;go fmt&lt;/code&gt;) because I found the rules to be simple enough to enforce: split standard libraries from everything else with an empty line in the &lt;code&gt;import&lt;/code&gt; statement, then sort the two batches lexicographically.&lt;/p&gt;

&lt;p&gt;I usually use &lt;a href=&#34;https://github.com/Masterminds/glide&#34;&gt;glide&lt;/a&gt; to manage the &lt;code&gt;/vendor&lt;/code&gt; folder in a project. With &lt;em&gt;glide&lt;/em&gt;, you can run &lt;code&gt;go fmt $(glide novendor)&lt;/code&gt; to format all source files except for the ones in &lt;code&gt;/vendor&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go vet&lt;/code&gt; is another helpful tool detect &amp;ldquo;common&amp;rdquo; Golang mistakes. It works by running a &lt;a href=&#34;https://golang.org/cmd/vet/&#34;&gt;set of heuristics&lt;/a&gt; against the source code and reporting the results. Note that it might return false positives, in the end, it&amp;rsquo;s up to you to decide what is important or not.
&lt;code&gt;govet&lt;/code&gt; expects Go package names as parameters so, to run it on all your project except for vendored dependencies, use &lt;code&gt;go vet $(go list ./... | grep -v /vendor/)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/golang/lint&#34;&gt;golint&lt;/a&gt; detects and reports coding style mistakes, it does not rewrite the code as &lt;code&gt;go fmt&lt;/code&gt; does. It is used internally by Google and it is a good base for your company&amp;rsquo;s internal Go style guide. Use &lt;code&gt;go list ./... | grep -v /vendor/ | xargs -L1 golint&lt;/code&gt; to skip the vendored dependencies.&lt;/p&gt;

&lt;p&gt;The Go compiler is very diligent to panic when you import a package you don&amp;rsquo;t use or when you define a variable you don&amp;rsquo;t use, but it doesn&amp;rsquo;t complain when you forget to handle the error returned by a function. For that, I use [errcheck]((&lt;a href=&#34;https://github.com/kisielk/errcheck&#34;&gt;https://github.com/kisielk/errcheck&lt;/a&gt;) and I find it invaluable.&lt;/p&gt;

&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;Much ink has been spilled on how to test code and, in particular, Go code. I am not going to go into details here, but do check out the excellent &lt;a href=&#34;https://godoc.org/testing&#34;&gt;testing package docs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When developing a service from scratch, I usually start with acceptance tests. It&amp;rsquo;s important to get them right because they help define the API and are going to be used for the whole lifetime of the project.
When the API protocol matures, I move down to integration and unit tests, often accompanied by redesigns and rewrites of the implementation.&lt;/p&gt;

&lt;p&gt;To ensure that tests only run against the public API of a module, I group tests in a &lt;code&gt;&amp;lt;module&amp;gt;_test&lt;/code&gt; package which has the added benefit of helping define a testable API, enforces dependency injection, component orthogonality, and single responsibility.&lt;/p&gt;

&lt;p&gt;I find testing to be hard! It requires design and maintenance and should not be considered second-class citizen to production code. It usually takes more time to write tests and significantly more code, but I consider this to be the norm and a mark of craftsmanship.&lt;/p&gt;

&lt;p&gt;To run a specific test in your suite, use &lt;code&gt;go test -run=&amp;lt;regexp/matching/test/func/name&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;benchmarking&#34;&gt;Benchmarking&lt;/h2&gt;

&lt;p&gt;Go has inbuilt support for micro-benchmarks. These are similar to unit tests, that is benchmarks measure the performance (time and memory allocations) of individual functions.
The way this works is that the body of the benchmark is executed enough times to get a statistically relevant measurement of its duration. One word of warning, different runs of the same benchmark function might yield significantly different results if run inside VMs or containers, especially on laptops which perform various power saving and cooling functions. See this &lt;a href=&#34;http://thornydev.blogspot.co.uk/2015/07/an-exercise-in-profiling-go-program.html&#34;&gt;blog post&lt;/a&gt; on the performance gains attainable.&lt;/p&gt;

&lt;p&gt;The Go benchmark framework is part of the &lt;a href=&#34;https://godoc.org/testing&#34;&gt;testing&lt;/a&gt; package. All benchmarks are executed sequentially, but you can run them in parallel with the &lt;code&gt;RunParallel&lt;/code&gt; helper function. This will create multiple goroutines and equally distribute the benchmark iterations amongst them. It&amp;rsquo;s also very effective at detecting race conditions and measuring lock contention.&lt;/p&gt;

&lt;p&gt;For some reason, tests are executed by default when running benchmarks so, to ignore the tests, run &lt;code&gt;go test -run=^$ -bench=.&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;race-detection&#34;&gt;Race Detection&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/doc/articles/race_detector.html&#34;&gt;Race detection&lt;/a&gt; is integrated into the Go compiler and is available for &lt;code&gt;go test -race&lt;/code&gt;, &lt;code&gt;go run -race&lt;/code&gt;, &lt;code&gt;go build -race&lt;/code&gt; and &lt;code&gt;go install -race&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It works by instrumenting every memory access such that when two goroutines read from/write to the same memory address, the program panics. Because of this design, data races can only be detected at runtime and only on code paths that are executed. This &lt;a href=&#34;https://github.com/google/sanitizers/wiki/ThreadSanitizerAlgorithm&#34;&gt;document&lt;/a&gt; describes how the instrumentation algorithm works in detail.&lt;/p&gt;

&lt;p&gt;The best way to detect data races is under realistic production conditions but this approach has a significant performance penalty. Running one instrumented binary in parallel with other non-instrumented binaries in a load-balanced set is a useful approach, especially for alpha stage services.&lt;/p&gt;

&lt;p&gt;Other ways to detect races are:
- running parallel benchmarks with the &lt;code&gt;-race&lt;/code&gt; flag can catch races with the right test cases.
- running parallel tests. Go doesn&amp;rsquo;t offer inbuilt support for parallel tests, so you will have to write the framework to do that yourself. In &lt;a href=&#34;https://github.com/bradfitz/talk-yapc-asia-2015/blob/master/talk.md#race-detector&#34;&gt;this example&lt;/a&gt; by Brad Fitzpatrick, multiple goroutines perform the same action under test and a data race is detected.
- another very good alternative is to run load tests: produce a high volume of requests against your race instrumented binary running in a staging or testing environment. For HTTP services I prefer to use  &lt;a href=&#34;http://github.com/tsenart/vegeta&#34;&gt;vegeta&lt;/a&gt; to perform load testing.&lt;/p&gt;

&lt;p&gt;Typical race conditions are accidentally shared variables, global variables accessed from multiple goroutines, etc. See this documentation article on &lt;a href=&#34;https://golang.org/doc/articles/race_detector.html#Typical_Data_Races&#34;&gt;tipical data race situations&lt;/a&gt;. Fixing these cases requires the use of synchronization primitives, such as those available in the &lt;code&gt;sync&lt;/code&gt; packages or channels to pass control to memory spaces.&lt;/p&gt;

&lt;p&gt;One thing to note is that because of Go&amp;rsquo;s memory model, not even read/writes on primitive data types are &lt;em&gt;not&lt;/em&gt; atomic. The &lt;code&gt;sync/atomic&lt;/code&gt; package has operations to perform atomic reads, atomic writes and atomic “compare and swaps” on ints and pointers.&lt;/p&gt;

&lt;h2 id=&#34;profiling&#34;&gt;Profiling&lt;/h2&gt;

&lt;p&gt;Profiling is also built-in with the Go tools. All types of profilers work in a similar way. An instrumented binary will spawn a separate thread which will send a signal to the main thread at a set interval. That signal will prompt the main thread to respond with data depending on the type of profiling done. The profile thread collects all this data and produces a profile report in binary format, essentially a collection of &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf objects&lt;/a&gt;, which you can save to disk for analysis using the &lt;code&gt;pprof&lt;/code&gt; tool.&lt;/p&gt;

&lt;p&gt;All profilers are based on sampling, that is they collect data every set period of time, then aggregate it:
- CPU profiling is useful for detecting slow running functions. It works by collecting stack traces of all currently running goroutines at a high frequency, then cumulating the times spent in each function on the stack. Very fast or infrequently called functions are discarded.
- Memory profiling is useful to detect functions which allocate relatively large chunks memory.  It works by collecting heap dumps from each running goroutine at high frequency and diffing sequential heap snapshots to count allocations. High memory consumption negatively affects performance because it means Go&amp;rsquo;s &amp;ldquo;stop-the-world&amp;rdquo; garbage collector has more work to do.
- Block profiling is used to detect lock contention between multiple goroutines and is done by recording when and where each goroutine is blocking.&lt;/p&gt;

&lt;p&gt;There are multiple ways to produce profiles.&lt;/p&gt;

&lt;h3 id=&#34;compiling-an-instrumented-binary&#34;&gt;Compiling an instrumented binary&lt;/h3&gt;

&lt;p&gt;For CPU profiling, you need to use the &lt;a href=&#34;https://godoc.org/runtime/pprof&#34;&gt;runtime/pprof&lt;/a&gt; package. Here&amp;rsquo;s an example of how you might instrument your &lt;code&gt;func main()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;fp = os.Create(profilePath)
defer fp.Close()
pprof.StartCPUProfile(fp) // start collecting data.
defer pprof.StopCPUProfile() // when the process stops, stop the profiler and store the data.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For memory profiling, you need the &lt;a href=&#34;https://godoc.org/runtime&#34;&gt;runtime&lt;/a&gt; and &lt;a href=&#34;https://godoc.org/runtime/pprof&#34;&gt;runtime/pprof&lt;/a&gt; packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;fp = os.Create(profilePath)
defer fp.Close()
runtime.GC() // trigger a run of the GC to get a clean heap snapshot.
defer pprof.WriteHeapProfile(fp) // when the process stops, stop the profiler and store the data.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For block profiling, you must use the &lt;a href=&#34;https://godoc.org/runtime&#34;&gt;runtime&lt;/a&gt; package. See &lt;a href=&#34;https://github.com/golang/go/issues/14689&#34;&gt;this GitHub issue&lt;/a&gt; on how to build a block profile.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;fp = os.Create(profilePath)
defer fp.Close()
runtime.SetBlockProfileRate(1) // catch every single block event.
profile = pprof.Lookup(&amp;quot;block&amp;quot;) // fetch the block profile singleton.
defer profile.WriteTo(fp) // write the profile data before stopping the process.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A couple of observations: first, errors are not being handled in the code above for simplicity reasons, you should definitely do so in your own code; second, the defer statements might not execute, depending on how your program stops, this will lead to the profile information not being transferred from memory to disk.&lt;/p&gt;

&lt;h3 id=&#34;instrument-tests-and-benchmarks&#34;&gt;Instrument tests and benchmarks&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;go test&lt;/code&gt; allows you to pass &lt;code&gt;-cpuprofile=&amp;lt;path/to/store/profile&amp;gt;&lt;/code&gt;, &lt;code&gt;-memprofile=&amp;lt;path/to/store/profile&amp;gt;&lt;/code&gt; or &lt;code&gt;-blockprofile=&amp;lt;path/to/store/profile&amp;gt;&lt;/code&gt; params. These options will build a test or benchmark binary instrumented for the specific profiler, run it and store the profiler report in the specified path at the end of the tests.
This method has the disadvantage that the profile report will contain data on the testing infrastructure which might have significant overhead.&lt;/p&gt;

&lt;h3 id=&#34;instrument-http-servers&#34;&gt;Instrument HTTP servers&lt;/h3&gt;

&lt;p&gt;For HTTP services, the &lt;code&gt;net/http/pprof&lt;/code&gt; package can be imported to produce profiles on demand. It works by registering a &lt;code&gt;/debug/pprof&lt;/code&gt; path on the default &lt;code&gt;net/http&lt;/code&gt; handler (you can also add it to custom HTTP handlers) which lists all the profiles it can produce.&lt;/p&gt;

&lt;p&gt;You may want to protect this URL with an authentication mechanism, profiling will significantly decrease the performance of your service and could potentially be used in a DOS attack.
When a specific profile path is requested, say &lt;code&gt;/debug/pprof/profile&lt;/code&gt;, the package starts profiling and writes the output on the HTTP stream to the client.
The &lt;code&gt;pprof&lt;/code&gt; tool can be used to consume the profile stream from an URL directly, but you can also download the profile locally.&lt;/p&gt;

&lt;h3 id=&#34;how-to-read-a-profile&#34;&gt;How to read a profile&lt;/h3&gt;

&lt;p&gt;The main way to use the profile data is with the built-in &lt;code&gt;pprof&lt;/code&gt; tool. This is based on the &lt;a href=&#34;https://github.com/google/pprof&#34;&gt;google/pprof&lt;/a&gt; project and is now bundled into the Go binary:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go tool pprof [&amp;lt;path/to/binary&amp;gt;] &amp;lt;path/to/profile&amp;gt;
pprof&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will start a &lt;code&gt;pprof&lt;/code&gt; session, which loads up all the profile data and exposes several useful commands:
- &lt;code&gt;top &amp;lt;N&amp;gt; -cum&lt;/code&gt; lists the top N functions, sorted such that the top listed function consumes the most CPU or memory or has the longest lock times, depending on profile under inspection.
- &lt;code&gt;list &amp;lt;function&amp;gt;&lt;/code&gt; lists the source code of a function and the lines that are the biggest contributors to resource consumption. You can specify a package name as a prefix to further filter the results. This command needs the binary file to be passed to the &lt;code&gt;pprof&lt;/code&gt; tool in order to produce source code listings.
- &lt;code&gt;web&lt;/code&gt; opens a browser window containing an SVG image of the execution graph for the profiled program and the time spent or memory consumed in each call. Note that the graph is pruned, only the top offenders are displayed!
Usually, you start with &lt;code&gt;top&lt;/code&gt; to find suspicious functions then dig deeper with &lt;code&gt;list&lt;/code&gt;. &lt;a href=&#34;https://github.com/bradfitz/talk-yapc-asia-2015/blob/master/talk.md#cpu-profiling&#34;&gt;This&lt;/a&gt; is an example of how the output of these commands looks like.&lt;/p&gt;

&lt;p&gt;When analyzing a memory profile with &lt;code&gt;pprof&lt;/code&gt;, make sure to add the &lt;code&gt;-alloc_space&lt;/code&gt; flag which displays allocated memory sizes, it will make &lt;code&gt;list&lt;/code&gt; better.&lt;/p&gt;

&lt;p&gt;For CPU profiles, a helpful tool I use is &lt;a href=&#34;https://github.com/uber/go-torch&#34;&gt;go-torch&lt;/a&gt;. It works by generating a flame graph, using &lt;a href=&#34;https://github.com/brendangregg/FlameGraph&#34;&gt;brendangregg/FlameGraph&lt;/a&gt; from a profile binary. To produce a flame graph SVG run &lt;code&gt;go-torch &amp;lt;binary&amp;gt; &amp;lt;profile&amp;gt;&lt;/code&gt;, then open it in a browser window.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Discovering these tools, understanding how they work, how to configure and use them is hard work! Once you have them in place, you still don&amp;rsquo;t get a free lunch, often times you need to understand the design decisions in the language implementation, the standard library, or in the third party libraries you are using. The standard library, in particular, has a lot of helper functions which boost productivity but are seldom efficient compared to their low-level counterparts.
That being said, the level of insight you can get into Go programs with just the built-in tools is impressive and the ecosystem around the language nicely complements its native capabilities.&lt;/p&gt;

&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Brad Fitzpatrick&amp;rsquo;s YAPC 2015 talk &amp;ldquo;Profiling and Optimizing Go&amp;rdquo; &lt;a href=&#34;https://www.youtube.com/watch?v=xxDZuPEgbBU&#34;&gt;youtube.com&lt;/a&gt;, &lt;a href=&#34;https://docs.google.com/presentation/d/1lL7Wlh9GBtTSieqHGJ5AUd1XVYR48UPhEloVem-79mA/view#slide=id.p&#34;&gt;slides&lt;/a&gt; and &lt;a href=&#34;https://github.com/bradfitz/talk-yapc-asia-2015/blob/master/talk.md&#34;&gt;writeup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Francisc Campoy&amp;rsquo;s &amp;ldquo;Go tooling in Action&amp;rdquo; &lt;a href=&#34;https://www.youtube.com/watch?v=uBjoTxosSys&#34;&gt;youtube.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Michael Peterson&amp;rsquo;s &amp;ldquo;An Exercise in Profiling a Go Program&amp;rdquo; &lt;a href=&#34;http://thornydev.blogspot.co.uk/2015/07/an-exercise-in-profiling-go-program.html&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Not updated but still good, Golang&amp;rsquo;s blog post &amp;ldquo;Profiling Go Programs&amp;rdquo; &lt;a href=&#34;https://blog.golang.org/profiling-go-programs&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>graphql with go and postgresql</title>
      <link>http://alexandrutopliceanu.ro/post/graphql-with-go-and-postgresql/</link>
      <pubDate>Sun, 22 Jan 2017 08:28:53 +0000</pubDate>
      
      <guid>http://alexandrutopliceanu.ro/post/graphql-with-go-and-postgresql/</guid>
      <description>

&lt;h1 id=&#34;graphql-in-use-building-a-blogging-engine-api-with-golang-and-postgresql&#34;&gt;GraphQL In Use: Building a Blogging Engine API with Golang and PostgreSQL&lt;/h1&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;GraphQL appears hard to use in production: the graph interface is flexible in its modeling capabilities but is a poor match for relational storage, both in terms of implementation and performance.&lt;/p&gt;

&lt;p&gt;In this document, we will design and write a simple blogging engine API, with the following specification:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;three types of resources (users, posts and comments) supporting a varied set of functionality (create a user, create a post, add a comment to a post, follow posts and comments from another user, etc.)&lt;/li&gt;
&lt;li&gt;use PostgreSQL as the backing data store (chosen because it&amp;rsquo;s a popular relational DB)&lt;/li&gt;
&lt;li&gt;write the API implementation in Golang (a popular language for writing APIs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will compare a simple GraphQL implementation with a pure REST alternative in terms of implementation complexity and efficiency for a common scenario: rendering a blog post page.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;GraphQL is an IDL (Interface Definition Language), designers define data types and model information as a graph. Each vertex is an instance of a data type, while edges represent relationships between nodes. This approach is flexible and can accommodate any business domain. However, the problem is that the design process is more complex and traditional data stores don&amp;rsquo;t map well to the graph model. See &lt;em&gt;Appendix 1&lt;/em&gt; for more details on this topic.&lt;/p&gt;

&lt;p&gt;GraphQL has been first proposed in 2014 by the Facebook Engineering Team. Although interesting and compelling in its advantages and features, it hasn&amp;rsquo;t seen mass adoption. Developers have to trade REST&amp;rsquo;s simplicity of design, familiarity and rich tooling for GraphQL&amp;rsquo;s flexibility of not being limited to just CRUD and network efficiency (it optimizes for round-trips to the server).&lt;/p&gt;

&lt;p&gt;Most walkthroughs and tutorials on GraphQL avoid the problem of fetching data from the data store to resolve queries. That is, how to design a database using general-purpose, popular storage solutions (like relational databases) to support efficient data retrieval for a GraphQL API.&lt;/p&gt;

&lt;p&gt;This document goes through building a blog engine GraphQL API. It is moderately complex in its functionality. It is scoped to a familiar business domain to facilitate comparisons with a REST based approach.&lt;/p&gt;

&lt;p&gt;The structure of this document is the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;in the first part we will design a GraphQL schema and explain some of features of the language that are used.&lt;/li&gt;
&lt;li&gt;next is the design of the PostgreSQL database in section two.&lt;/li&gt;
&lt;li&gt;part three covers the Golang implementation of the GraphQL schema designed in part one.&lt;/li&gt;
&lt;li&gt;in part four we compare the task of rendering a blog post page from the perspective of fetching the needed data from the backend.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;related&#34;&gt;Related&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The excellent &lt;a href=&#34;http://graphql.org/learn/&#34;&gt;GraphQL introduction document&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The complete and working code for this project is on &lt;a href=&#34;https://github.com/topliceanu/graphql-go-example&#34;&gt;github.com/topliceanu/graphql-go-example&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;modeling-a-blog-engine-in-graphql&#34;&gt;Modeling a blog engine in GraphQL&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Listing 1&lt;/em&gt; contains the entire schema for the blog engine API. It shows the data types of the vertices composing the graph. The relationships between vertices, ie. the edges, are modeled as attributes of a given type.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-graphql&#34;&gt;type User {
  id: ID
  email: String!
  post(id: ID!): Post
  posts: [Post!]!
  follower(id: ID!): User
  followers: [User!]!
  followee(id: ID!): User
  followees: [User!]!
}

type Post {
  id: ID
  user: User!
  title: String!
  body: String!
  comment(id: ID!): Comment
  comments: [Comment!]!
}

type Comment {
  id: ID
  user: User!
  post: Post!
  title: String
  body: String!
}

type Query {
  user(id: ID!): User
}

type Mutation {
  createUser(email: String!): User
  removeUser(id: ID!): Boolean
  follow(follower: ID!, followee: ID!): Boolean
  unfollow(follower: ID!, followee: ID!): Boolean
  createPost(user: ID!, title: String!, body: String!): Post
  removePost(id: ID!): Boolean
  createComment(user: ID!, post: ID!, title: String!, body: String!): Comment
  removeComment(id: ID!): Boolean
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Listing 1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The schema is written in the GraphQL DSL, which is used for defining custom data types, such as &lt;code&gt;User&lt;/code&gt;, &lt;code&gt;Post&lt;/code&gt; and &lt;code&gt;Comment&lt;/code&gt;. A set of primitive data types is also provided by the language, such as &lt;code&gt;String&lt;/code&gt;, &lt;code&gt;Boolean&lt;/code&gt; and &lt;code&gt;ID&lt;/code&gt; (which is an alias of &lt;code&gt;String&lt;/code&gt; with the additional semantics of being the unique identifier of a vertex).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Query&lt;/code&gt; and &lt;code&gt;Mutation&lt;/code&gt; are optional types recognized by the parser and used in querying the graph. Reading data from a GraphQL API is equivalent to traversing the graph. As such a starting vertex needs to be provided; this role is fulfilled by the &lt;code&gt;Query&lt;/code&gt; type. In this case, all queries to the graph must start with a user specified by id &lt;code&gt;user(id:ID!)&lt;/code&gt;. For writing data, the &lt;code&gt;Mutation&lt;/code&gt; vertex type is defined. This exposes a set of operations, modeled as parameterized attributes which traverse (and return) the newly created vertex types. See &lt;em&gt;Listing 2&lt;/em&gt; for examples of how these queries might look.&lt;/p&gt;

&lt;p&gt;Vertex attributes can be parameterized, ie. accept arguments. In the context of graph traversal, if a post vertex has multiple comment vertices, you can traverse just one of them by specifying &lt;code&gt;comment(id: ID)&lt;/code&gt;. All this is by design, the designer can choose not to provide direct paths to individual vertices.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;!&lt;/code&gt; character is a type post-fix, works for both primitive or user-defined types and has two semantics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;when used for the type of a param in a parametriezed attribute, it means that the param is required.&lt;/li&gt;
&lt;li&gt;when used for the return type of an attribute it means that the attribute will not be null when the vertex is retrieved.&lt;/li&gt;
&lt;li&gt;combinations are possible, for instance &lt;code&gt;[Comment!]!&lt;/code&gt; represents a list of non-null Comment vertices, where &lt;code&gt;[]&lt;/code&gt;, &lt;code&gt;[Comment]&lt;/code&gt; are valid, but &lt;code&gt;null, [null], [Comment, null]&lt;/code&gt; are not.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Listing 2&lt;/em&gt; contains a list of &lt;em&gt;curl&lt;/em&gt; commands against the blogging API which will populate the graph using mutations and then query it to retrieve data. To run them, follow the instructions in the &lt;a href=&#34;https://github.com/topliceanu/graphql-go-example&#34;&gt;topliceanu/graphql-go-example&lt;/a&gt; repo to build and run the service.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Mutations to create users 1,2 and 3. Mutations also work as queries, in these cases we retrieve the ids and emails of the newly created users.
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createUser(email:&amp;quot;user1@x.co&amp;quot;){id, email}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createUser(email:&amp;quot;user2@x.co&amp;quot;){id, email}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createUser(email:&amp;quot;user3@x.co&amp;quot;){id, email}}&#39;
# Mutations to add posts for the users. We retrieve their ids to comply with the schema, otherwise we will get an error.
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createPost(user:1,title:&amp;quot;post1&amp;quot;,body:&amp;quot;body1&amp;quot;){id}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createPost(user:1,title:&amp;quot;post2&amp;quot;,body:&amp;quot;body2&amp;quot;){id}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createPost(user:2,title:&amp;quot;post3&amp;quot;,body:&amp;quot;body3&amp;quot;){id}}&#39;
# Mutations to all comments to posts. `createComment` expects the user&#39;s ID, a title and a body. See the schema in Listing 1.
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createComment(user:2,post:1,title:&amp;quot;comment1&amp;quot;,body:&amp;quot;comment1&amp;quot;){id}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createComment(user:1,post:3,title:&amp;quot;comment2&amp;quot;,body:&amp;quot;comment2&amp;quot;){id}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {createComment(user:3,post:3,title:&amp;quot;comment3&amp;quot;,body:&amp;quot;comment3&amp;quot;){id}}&#39;
# Mutations to have the user3 follow users 1 and 2. Note that the `follow` mutation only returns a boolean which doesn&#39;t need to be specified.
curl -XPOST http://vm:8080/graphql -d &#39;mutation {follow(follower:3, followee:1)}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;mutation {follow(follower:3, followee:2)}&#39;

# Query to fetch all data for user 1
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:1)}&#39;
# Queries to fetch the followers of user2 and, respectively, user1.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:2){followers{id, email}}}&#39;
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:1){followers{id, email}}}&#39;
# Query to check if user2 is being followed by user1. If so retrieve user1&#39;s email, otherwise return null.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:2){follower(id:1){email}}}&#39;
# Query to return ids and emails for all the users being followed by user3.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:3){followees{id, email}}}&#39;
# Query to retrieve the email of user3 if it is being followed by user1.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:1){followee(id:3){email}}}&#39;
# Query to fetch user1&#39;s post2 and retrieve the title and body. If post2 was not created by user1, null will be returned.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:1){post(id:2){title,body}}}&#39;
# Query to retrieve all data about all the posts of user1.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:1){posts{id,title,body}}}&#39;
# Query to retrieve the user who wrote post2, if post2 was written by user1; a contrived example that displays the flexibility of the language.
curl -XPOST http://vm:8080/graphql -d &#39;{user(id:1){post(id:2){user{id,email}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Listing 2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By carefully desiging the mutations and type attributes, powerful and expressive queries are possible.&lt;/p&gt;

&lt;h2 id=&#34;designing-the-postgresql-database&#34;&gt;Designing the PostgreSQL database&lt;/h2&gt;

&lt;p&gt;The relational database design is, as usual, driven by the need to avoid data duplication. This approach was chosen for two reasons:
 1. to show that there is no need for a specialized database technology or to learn and use new design techniques to accommodate a GraphQL API.
 2. to show that a GraphQL API can still be created on top of existing databases, more specifically databases originally designed to power REST endpoints or even traditional server-side rendered HTML websites.&lt;/p&gt;

&lt;p&gt;See &lt;em&gt;Appendix 1&lt;/em&gt; for a discussion on differences between relational and graph databases with respect to building a GraphQL API. &lt;em&gt;Listing 3&lt;/em&gt; shows the SQL commands to create the new database. The database schema generally matches the GraphQL schema. The &lt;code&gt;followers&lt;/code&gt; relation needed to be added to support the &lt;code&gt;follow/unfollow&lt;/code&gt; mutations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE IF NOT EXISTS users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(100) NOT NULL
);
CREATE TABLE IF NOT EXISTS posts (
  id SERIAL PRIMARY KEY,
  user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  title VARCHAR(200) NOT NULL,
  body TEXT NOT NULL
);
CREATE TABLE IF NOT EXISTS comments (
  id SERIAL PRIMARY KEY,
  user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
  title VARCHAR(200) NOT NULL,
  body TEXT NOT NULL
);
CREATE TABLE IF NOT EXISTS followers (
  follower_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  followee_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  PRIMARY KEY(follower_id, followee_id)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Listing 3&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;golang-api-implementation&#34;&gt;Golang API Implementation&lt;/h2&gt;

&lt;p&gt;The GraphQL parser implemented in Go and used in this project is &lt;code&gt;github.com/graphql-go/graphql&lt;/code&gt;. It contains a query parser, but no schema parser. This requires the programmer to build the GraphQL schema in Go using the constructs offered by the library. This is unlike the reference &lt;a href=&#34;https://github.com/graphql/graphql-js&#34;&gt;nodejs implementation&lt;/a&gt;, which offers a schema parser and exposes hooks for data fetching. As such the schema in &lt;code&gt;Listing 1&lt;/code&gt; is only useful as a guideline and has to be translated into Golang code. However, this &lt;em&gt;&amp;ldquo;limitation&amp;rdquo;&lt;/em&gt; offers the opportunity to peer behind the levels of abstraction and see how the schema relates to the graph traversal model for retrieving data. &lt;em&gt;Listing 4&lt;/em&gt; shows the implementation of the &lt;code&gt;Comment&lt;/code&gt; vertex type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var CommentType = graphql.NewObject(graphql.ObjectConfig{
	Name: &amp;quot;Comment&amp;quot;,
	Fields: graphql.Fields{
		&amp;quot;id&amp;quot;: &amp;amp;graphql.Field{
			Type: graphql.NewNonNull(graphql.ID),
			Resolve: func(p graphql.ResolveParams) (interface{}, error) {
				if comment, ok := p.Source.(*Comment); ok == true {
					return comment.ID, nil
				}
				return nil, nil
			},
		},
		&amp;quot;title&amp;quot;: &amp;amp;graphql.Field{
			Type: graphql.NewNonNull(graphql.String),
			Resolve: func(p graphql.ResolveParams) (interface{}, error) {
				if comment, ok := p.Source.(*Comment); ok == true {
					return comment.Title, nil
				}
				return nil, nil
			},
		},
		&amp;quot;body&amp;quot;: &amp;amp;graphql.Field{
			Type: graphql.NewNonNull(graphql.ID),
			Resolve: func(p graphql.ResolveParams) (interface{}, error) {
				if comment, ok := p.Source.(*Comment); ok == true {
					return comment.Body, nil
				}
				return nil, nil
			},
		},
	},
})
func init() {
	CommentType.AddFieldConfig(&amp;quot;user&amp;quot;, &amp;amp;graphql.Field{
		Type: UserType,
		Resolve: func(p graphql.ResolveParams) (interface{}, error) {
			if comment, ok := p.Source.(*Comment); ok == true {
				return GetUserByID(comment.UserID)
			}
			return nil, nil
		},
	})
	CommentType.AddFieldConfig(&amp;quot;post&amp;quot;, &amp;amp;graphql.Field{
		Type: PostType,
		Args: graphql.FieldConfigArgument{
			&amp;quot;id&amp;quot;: &amp;amp;graphql.ArgumentConfig{
				Description: &amp;quot;Post ID&amp;quot;,
				Type:        graphql.NewNonNull(graphql.ID),
			},
		},
		Resolve: func(p graphql.ResolveParams) (interface{}, error) {
			i := p.Args[&amp;quot;id&amp;quot;].(string)
			id, err := strconv.Atoi(i)
			if err != nil {
				return nil, err
			}
			return GetPostByID(id)
		},
	})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Listing 4&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just like in the schema in &lt;em&gt;Listing 1&lt;/em&gt;, the &lt;code&gt;Comment&lt;/code&gt; type is a structure with three attributes defined statically; &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;body&lt;/code&gt;. Two other attributes &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; are defined dynamically to avoid circular dependencies.&lt;/p&gt;

&lt;p&gt;Go does not lend itself well to this kind of dynamic modeling, there is little type-checking support, most of the variables in the code are of type &lt;code&gt;interface{}&lt;/code&gt; and need to be type asserted before use. &lt;code&gt;CommentType&lt;/code&gt; itself is a variable of type &lt;code&gt;graphql.Object&lt;/code&gt; and its attributes are of type &lt;code&gt;graphql.Field&lt;/code&gt;. So, there&amp;rsquo;s no direct translation between the GraphQL DSL and the data structures used in Go.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;resolve&lt;/code&gt; function for each field exposes the &lt;code&gt;Source&lt;/code&gt; parameter which is a data type vertex representing the previous node in the traversal. All the attributes of a &lt;code&gt;Comment&lt;/code&gt; have, as source, the current &lt;code&gt;CommentType&lt;/code&gt; vertex. Retrieving the &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;body&lt;/code&gt; is  a straightforward attribute access, while retrieving the &lt;code&gt;user&lt;/code&gt; and the &lt;code&gt;post&lt;/code&gt; requires graph traversals, and thus database queries. The SQL queries are left out of this document because of their simplicity, but they are available in the github repository listed in the &lt;em&gt;References&lt;/em&gt; section.&lt;/p&gt;

&lt;h2 id=&#34;comparison-with-rest-in-common-scenarios&#34;&gt;Comparison with REST in common scenarios&lt;/h2&gt;

&lt;p&gt;In this section we will present a common blog page rendering scenario and compare the REST and the GraphQL implementations. The focus will be on the number of inbound/outbound requests, because these are the biggest contributors to the latency of rendering the page.&lt;/p&gt;

&lt;p&gt;The scenario: render a blog post page. It should contain information about the author (email), about the blog post (title, body), all comments (title, body) and whether the user that made the comment follows the author of the blog post or not. &lt;em&gt;Figure 1&lt;/em&gt; and &lt;em&gt;Figure 2&lt;/em&gt; show the interaction between the client SPA, the API server and the database, for a REST API and, respectively, for a GraphQL API.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+------+                  +------+                  +--------+
|client|                  |server|                  |database|
+--+---+                  +--+---+                  +----+---+
   |      GET /blogs/:id     |                           |
1. +-------------------------&amp;gt;  SELECT * FROM blogs...   |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   &amp;lt;-------------------------+                           |
   |                         |                           |
   |     GET /users/:id      |                           |
2. +-------------------------&amp;gt;  SELECT * FROM users...   |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   &amp;lt;-------------------------+                           |
   |                         |                           |
   | GET /blogs/:id/comments |                           |
3. +-------------------------&amp;gt; SELECT * FROM comments... |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   &amp;lt;-------------------------+                           |
   |                         |                           |
   | GET /users/:id/followers|                           |
4. +-------------------------&amp;gt; SELECT * FROM followers.. |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   &amp;lt;-------------------------+                           |
   |                         |                           |
   +                         +                           +
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+------+                  +------+                  +--------+
|client|                  |server|                  |database|
+--+---+                  +--+---+                  +----+---+
   |      GET /graphql       |                           |
1. +-------------------------&amp;gt;  SELECT * FROM blogs...   |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   |                         |                           |
   |                         |                           |
   |                         |                           |
2. |                         |  SELECT * FROM users...   |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   |                         |                           |
   |                         |                           |
   |                         |                           |
3. |                         | SELECT * FROM comments... |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   |                         |                           |
   |                         |                           |
   |                         |                           |
4. |                         | SELECT * FROM followers.. |
   |                         +---------------------------&amp;gt;
   |                         &amp;lt;---------------------------+
   &amp;lt;-------------------------+                           |
   |                         |                           |
   +                         +                           +
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Figure 2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Listing 5&lt;/em&gt; contains the single GraphQL query which will fetch all the data needed to render the blog post.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-graphql&#34;&gt;{
  user(id: 1) {
    email
    followers
    post(id: 1) {
      title
      body
      comments {
        id
        title
        user {
          id
          email
        }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Listing 5&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The number of queries to the database for this scenario is deliberately identical, but the number of HTTP requests to the API server has been reduced to just one. We argue that the HTTP requests over the Internet are the most costly in this type of application.&lt;/p&gt;

&lt;p&gt;The backend doesn&amp;rsquo;t have to be designed differently to start reaping the benefits of GraphQL, transitioning from REST to GraphQL can be done incrementally. This allows to measure performance improvements and optimize. From this point, the API developer can start to optimize (potentially merge) SQL queries to improve performance. The opportunity for caching is greatly increased, both on the database and API levels.&lt;/p&gt;

&lt;p&gt;Abstractions on top of SQL (for instance ORM layers) usually have to contend with the &lt;code&gt;n+1&lt;/code&gt; problem. In step &lt;code&gt;4.&lt;/code&gt; of the REST example, a client could have had to request the follower status for the author of each comment in separate requests. This is because in REST there is no standard way of expressing relationships between more than two resources, whereas GraphQL was designed to prevent this problem by using nested queries. Here, we cheat by fetching all the followers of the user. We defer to the client the logic of determining the users who commented and also followed the author.&lt;/p&gt;

&lt;p&gt;Another difference is fetching more data than the client needs, in order to not break the REST resource abstractions. This is important for bandwidth consumption and battery life spent parsing and storing unneeded data.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;GraphQL is a viable alternative to REST because:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;while it is more difficult to design the API, the process can be done incrementally. Also for this reason, it&amp;rsquo;s easy to transition from REST to GraphQL, the two paradigms can coexist without issues.&lt;/li&gt;
&lt;li&gt;it is more efficient in terms of network requests, even with naive implementations like the one in this document. It also offers more opportunities for query optimization and result caching.&lt;/li&gt;
&lt;li&gt;it is more efficient in terms of bandwidth consumption and CPU cycles spent parsing results, because it only returns what is needed to render the page.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;REST remains very useful if:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;your API is simple, either has a low number of resources or simple relationships between them.&lt;/li&gt;
&lt;li&gt;you already work with REST APIs inside your organization and you have the tooling all set up or your clients expect REST APIs from your organization.&lt;/li&gt;
&lt;li&gt;you have complex ACL policies. In the blog example, a potential feature could allow users fine-grained control over who can see their email, their posts, their comments on a particular post, whom they follow etc. Optimizing data retrieval while checking complex business rules can be more difficult.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;appendix-1-graph-databases-and-efficient-data-storage&#34;&gt;Appendix 1: Graph Databases And Efficient Data Storage&lt;/h2&gt;

&lt;p&gt;While it is intuitive to think about application domain data as a graph, as this document demonstrates, the question of efficient data storage to support such an interface is still open.&lt;/p&gt;

&lt;p&gt;In recent years graph databases have become more popular. Deferring the complexity of resolving the request by translating the GraphQL query into a specific graph database query language seems like a viable solution.&lt;/p&gt;

&lt;p&gt;The problem is that graphs are not an efficient data structure compared to relational databases. A vertex can have links to any other vertex in the graph and access patterns are less predictable and thus offer less opportunity for optimization.&lt;/p&gt;

&lt;p&gt;For instance, the problem of caching, ie. which vertices need to be kept in memory for fast access? Generic caching algorithms may not be very efficient in the context of graph traversal.&lt;/p&gt;

&lt;p&gt;The problem of database sharding: splitting the database into smaller, non-interacting databases, living on separate hardware. In academia, the problem of splitting a graph on the minimal cut is well understood but it is suboptimal and may potentially result in highly unbalanced cuts due to pathological worst-case scenarios.&lt;/p&gt;

&lt;p&gt;With relational databases, data is modeled in records (or rows, or tuples) and columns, tables and database names are simply namespaces. Most databases are row-oriented, which means that each record is a contiguous chunk of memory, all records in a table are neatly packed one after the other on the disk (usually sorted by some key column). This is efficient because it is optimal for the way physical storage works. The most expensive operation for an HDD is to move the read/write head to another sector on the disk, so minimizing these accesses is critical.&lt;/p&gt;

&lt;p&gt;There is also a high probability that, if the application is interested in a particular record, it will need the whole record, not just a single key from it. There is a high probabilty that if the application is interested in a record, it will be interested in its neighbours as well, for instance a table scan. These two observations make relational databases quite efficient. However, for this reason also, the worst use-case scenario for a relational database is random access across all data all the time. This is exactly what graph databases do.&lt;/p&gt;

&lt;p&gt;With the advent of SSD drives which have faster random access, cheap RAM memory which makes caching large portions of a graph database possible, better techniques to optimize graph caching and partitioning, graph databases have become a viable storage solution. And most large companies use it: Facebook has the Social Graph, Google has the Knowledge Graph.&lt;/p&gt;

&lt;p&gt;Comments on &lt;a href=&#34;https://news.ycombinator.com/item?id=13454333&#34;&gt;HackerNews&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>